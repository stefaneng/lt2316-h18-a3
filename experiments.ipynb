{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "loading annotations into memory...\n",
      "Done (t=13.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.91s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import mycoco\n",
    "\n",
    "mycoco.setmode('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = mycoco.query([['']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x7f5bd15c1470>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/tokenizer10000.pickle', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "    \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compile() missing 1 required positional argument: 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-7224cb392277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./models/encoder.model.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: compile() missing 1 required positional argument: 'optimizer'"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "encoder = load_model('./models/encoder.model.hdf5')\n",
    "encoder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_examples = mycoco.iter_captions_examples(all_ids, tokenizer, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)],\n",
       " array([[0.46396238, 0.23454374, 0.        , ..., 0.08909912, 0.31195068,\n",
       "         0.29884964]], dtype=float32)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(cap_examples)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 5, 50)        500000      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 50)           20200       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "word_prediction (Dense)         (None, 10000)        510000      lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "image_vec_prediction (Dense)    (None, 5000)         255000      lstm_6[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,285,200\n",
      "Trainable params: 1,285,200\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, LSTM, Embedding, Input\n",
    "\n",
    "input_length = 5\n",
    "embed_size = 50\n",
    "vocab_size = 10000\n",
    "img_vec_size = 5000\n",
    "\n",
    "inputs = Input(shape=(input_length,))\n",
    "embed = Embedding(vocab_size, embed_size, input_length=input_length)(inputs)\n",
    "lstm = LSTM(50, dropout=0.1)(embed)\n",
    "# Word prediction softmax\n",
    "word_pred = Dense(vocab_size, activation='softmax', name='word_prediction')(lstm)\n",
    "image_vec_preds = Dense(img_vec_size, activation = 'sigmoid', name='image_vec_prediction')(lstm)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and two Dense layers outputs\n",
    "model = Model(inputs=inputs, outputs=[word_pred, image_vec_preds])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "        loss={\n",
    "            'word_prediction': 'categorical_crossentropy',\n",
    "            'image_vec_prediction': 'binary_crossentropy'\n",
    "        },\n",
    "        metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 41/100 [===========>..................] - ETA: 2s - loss: 9.8874 - word_prediction_loss: 9.2016 - image_vec_prediction_loss: 0.6858 - word_prediction_acc: 0.0244 - image_vec_prediction_acc: 0.2743"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 30ms/step - loss: 9.5729 - word_prediction_loss: 8.9611 - image_vec_prediction_loss: 0.6118 - word_prediction_acc: 0.0500 - image_vec_prediction_acc: 0.3154\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 8.0058 - word_prediction_loss: 7.7042 - image_vec_prediction_loss: 0.3016 - word_prediction_acc: 0.0500 - image_vec_prediction_acc: 0.3550\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.9948 - word_prediction_loss: 7.3949 - image_vec_prediction_loss: 0.5998 - word_prediction_acc: 0.0800 - image_vec_prediction_acc: 0.2466\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.3477 - word_prediction_loss: 6.8338 - image_vec_prediction_loss: 0.5139 - word_prediction_acc: 0.0500 - image_vec_prediction_acc: 0.2932\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.3283 - word_prediction_loss: 6.8762 - image_vec_prediction_loss: 0.4521 - word_prediction_acc: 0.1300 - image_vec_prediction_acc: 0.3161\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.4749 - word_prediction_loss: 6.9993 - image_vec_prediction_loss: 0.4757 - word_prediction_acc: 0.0900 - image_vec_prediction_acc: 0.3165\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 6.8629 - word_prediction_loss: 6.3924 - image_vec_prediction_loss: 0.4705 - word_prediction_acc: 0.1600 - image_vec_prediction_acc: 0.2648\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.2029 - word_prediction_loss: 6.7136 - image_vec_prediction_loss: 0.4892 - word_prediction_acc: 0.0800 - image_vec_prediction_acc: 0.2985\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.3544 - word_prediction_loss: 6.8765 - image_vec_prediction_loss: 0.4779 - word_prediction_acc: 0.1100 - image_vec_prediction_acc: 0.2995\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.4617 - word_prediction_loss: 6.9917 - image_vec_prediction_loss: 0.4700 - word_prediction_acc: 0.1400 - image_vec_prediction_acc: 0.3256\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 6.9384 - word_prediction_loss: 6.5014 - image_vec_prediction_loss: 0.4371 - word_prediction_acc: 0.1700 - image_vec_prediction_acc: 0.3423\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.5685 - word_prediction_loss: 7.1221 - image_vec_prediction_loss: 0.4464 - word_prediction_acc: 0.0700 - image_vec_prediction_acc: 0.3286\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.2292 - word_prediction_loss: 6.7879 - image_vec_prediction_loss: 0.4412 - word_prediction_acc: 0.1700 - image_vec_prediction_acc: 0.3389\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.3639 - word_prediction_loss: 6.9271 - image_vec_prediction_loss: 0.4368 - word_prediction_acc: 0.0400 - image_vec_prediction_acc: 0.3536\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 7.3683 - word_prediction_loss: 6.9165 - image_vec_prediction_loss: 0.4518 - word_prediction_acc: 0.1200 - image_vec_prediction_acc: 0.3305\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 7.0245 - word_prediction_loss: 6.5506 - image_vec_prediction_loss: 0.4739 - word_prediction_acc: 0.0800 - image_vec_prediction_acc: 0.3180\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 6.9104 - word_prediction_loss: 6.4655 - image_vec_prediction_loss: 0.4449 - word_prediction_acc: 0.1400 - image_vec_prediction_acc: 0.3222\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 6.9815 - word_prediction_loss: 6.5710 - image_vec_prediction_loss: 0.4105 - word_prediction_acc: 0.1000 - image_vec_prediction_acc: 0.3590\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 7.2928 - word_prediction_loss: 6.8513 - image_vec_prediction_loss: 0.4415 - word_prediction_acc: 0.1100 - image_vec_prediction_acc: 0.3450\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 6.8554 - word_prediction_loss: 6.3809 - image_vec_prediction_loss: 0.4745 - word_prediction_acc: 0.0600 - image_vec_prediction_acc: 0.3040\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 6.8948 - word_prediction_loss: 6.4817 - image_vec_prediction_loss: 0.4131 - word_prediction_acc: 0.0600 - image_vec_prediction_acc: 0.3571\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 6.9989 - word_prediction_loss: 6.5351 - image_vec_prediction_loss: 0.4638 - word_prediction_acc: 0.0900 - image_vec_prediction_acc: 0.2999\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 6.5754 - word_prediction_loss: 6.1051 - image_vec_prediction_loss: 0.4703 - word_prediction_acc: 0.1200 - image_vec_prediction_acc: 0.3004\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 6.6207 - word_prediction_loss: 6.1642 - image_vec_prediction_loss: 0.4566 - word_prediction_acc: 0.1000 - image_vec_prediction_acc: 0.3129\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.2415 - word_prediction_loss: 6.8017 - image_vec_prediction_loss: 0.4398 - word_prediction_acc: 0.1400 - image_vec_prediction_acc: 0.3346\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.5900 - word_prediction_loss: 7.1392 - image_vec_prediction_loss: 0.4508 - word_prediction_acc: 0.0800 - image_vec_prediction_acc: 0.3200\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 7.6966 - word_prediction_loss: 7.2584 - image_vec_prediction_loss: 0.4382 - word_prediction_acc: 0.0700 - image_vec_prediction_acc: 0.3366\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 6.2365 - word_prediction_loss: 5.8347 - image_vec_prediction_loss: 0.4019 - word_prediction_acc: 0.1200 - image_vec_prediction_acc: 0.3676\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 6.6164 - word_prediction_loss: 6.2022 - image_vec_prediction_loss: 0.4142 - word_prediction_acc: 0.1000 - image_vec_prediction_acc: 0.3376\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 6.2509 - word_prediction_loss: 5.7674 - image_vec_prediction_loss: 0.4836 - word_prediction_acc: 0.1300 - image_vec_prediction_acc: 0.3161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5bd15c1128>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(cap_examples, steps_per_epoch=100, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
